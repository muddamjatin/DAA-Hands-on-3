{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvv7abHjIDDh8a+CiVwVL0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muddamjatin/DAA-Hands-on-3/blob/main/daa%20handson%204.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SBlGjMzKbTjb"
      },
      "outputs": [],
      "source": [
        "def fib(n):\n",
        "    if n == 0:\n",
        "        return 0\n",
        "    if n == 1:\n",
        "        return 1\n",
        "    return fib(n-1) + fib(n-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stepping into fib(5)\n",
        "To track the recursive calls, we follow the structure exactly as described. Below is the recursive call stack for fib(5):\n",
        "\n",
        "fib(5) calls fib(4) and fib(3)\n",
        "\n",
        "fib(4) calls fib(3) and fib(2)\n",
        "\n",
        "fib(3) calls fib(2) and fib(1)\n",
        "\n",
        "fib(2) calls fib(1) and fib(0)\n",
        "\n",
        "fib(1) returns 1\n",
        "\n",
        "fib(0) returns 0\n",
        "\n",
        "fib(2) returns 1 + 0 = 1\n",
        "\n",
        "fib(1) returns 1\n",
        "\n",
        "fib(3) returns 1 + 1 = 2\n",
        "\n",
        "fib(2) calls fib(1) and fib(0)\n",
        "\n",
        "fib(1) returns 1\n",
        "\n",
        "fib(0) returns 0\n",
        "\n",
        "fib(2) returns 1 + 0 = 1\n",
        "\n",
        "fib(4) returns 2 + 1 = 3\n",
        "\n",
        "fib(3) calls fib(2) and fib(1)\n",
        "\n",
        "fib(2) calls fib(1) and fib(0)\n",
        "\n",
        "fib(1) returns 1\n",
        "\n",
        "fib(0) returns 0\n",
        "\n",
        "fib(2) returns 1 + 0 = 1\n",
        "\n",
        "fib(1) returns 1\n",
        "\n",
        "fib(3) returns 1 + 1 = 2\n",
        "\n",
        "fib(5) returns 3 + 2 = 5\n"
      ],
      "metadata": {
        "id": "SeohxP_Ob9Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fib(5) -> fib(4) -> fib(3) -> fib(2) -> fib(1)\n",
        "#                     -> fib(2) -> fib(1)\n",
        "#               -> fib(3) -> fib(2) -> fib(1)\n",
        "#                      -> fib(1)\n",
        "#        -> fib(3) -> fib(2) -> fib(1)\n",
        "#                      -> fib(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ou28mcEAcBS_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1**: Merging K Sorted Arrays\n",
        "\n",
        "**Analysis of Time Complexity**\n",
        "O(K * N log K) is the temporal complexity, where:\n",
        "\n",
        "The number of arrays is K.\n",
        "The size of every array is N.\n",
        "We perform the heap operations (heappush and heappop) for each element, resulting in a total of K * N elements. These operations require O(log K) time.\n",
        "\n",
        "**Distributed Sorting**: To parallelize the merging operation across numerous machines, you can use distributed systems such as MapReduce for huge datasets. For very big K and N, this would greatly accelerate the process for gigantic input sizes.\n",
        "\n",
        "**The optimal block-wise merge**: It depends on cache efficiency and memory access patterns when combining big arrays. Performance can be enhanced by a block-wise merging that reduces cache misses, particularly on contemporary technology where memory access speed is a bottleneck."
      ],
      "metadata": {
        "id": "r60_KYEAdWvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "\n",
        "def merge_k_sorted_arrays(arrays):\n",
        "    min_heap = []\n",
        "\n",
        "    # Push the first element of each array along with the array index and element index\n",
        "    for i, array in enumerate(arrays):\n",
        "        if array:  # check if the array is non-empty\n",
        "            heapq.heappush(min_heap, (array[0], i, 0))\n",
        "\n",
        "    result = []\n",
        "\n",
        "    while min_heap:\n",
        "        value, array_index, element_index = heapq.heappop(min_heap)\n",
        "        result.append(value)\n",
        "\n",
        "        # If the next element in the same array exists, add it to the heap\n",
        "        if element_index + 1 < len(arrays[array_index]):\n",
        "            next_value = arrays[array_index][element_index + 1]\n",
        "            heapq.heappush(min_heap, (next_value, array_index, element_index + 1))\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "cegtg_WZcrmy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2: Remove Duplicates from a Sorted Array\n",
        "\n",
        "Time Complexity Analysis\n",
        "\n",
        "The time complexity is O(N), where N is the size of the input array. We only traverse the array once, and all operations inside the loop are O(1).\n",
        "\n",
        "Compressed Storage (Run-Length Encoding): Rather than eliminating duplicates, you can use run-length encoding (RLE) to represent the array in a compressed format if it has significant blocks of repeated elements. [2, 2, 2, 3, 4,] for instance, may be saved as [(2,3), (3,2), (4,1)], which would imply \"2 appears three times, 3 appears two times, and 4 appears once.\" This makes storage more effective, but it also necessitates changes to array operations.\n",
        "\n",
        "Adaptive Algorithm for Larger Duplicates: You may modify the algorithm to look for large blocks of duplicates and treat them in bulk, eliminating single element comparisons, if you anticipate more duplicates at the beginning of the array (such as in situations where duplicates are clustered). This could lessen"
      ],
      "metadata": {
        "id": "x_rCrdOrd5eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates(arr):\n",
        "    if not arr:\n",
        "        return []\n",
        "\n",
        "    # Initialize the write index to the first element\n",
        "    write_index = 1\n",
        "\n",
        "    # Traverse the array starting from the second element\n",
        "    for i in range(1, len(arr)):\n",
        "        if arr[i] != arr[i - 1]:\n",
        "            arr[write_index] = arr[i]\n",
        "            write_index += 1\n",
        "\n",
        "    # Return the array up to the point where unique elements were written\n",
        "    return arr[:write_index]"
      ],
      "metadata": {
        "id": "WE01xWYJdbtM"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}